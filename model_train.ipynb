{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9860b17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (541909, 8)\n",
      "Dataset columns:\n",
      "['InvoiceNo', 'StockCode', 'Description', 'Quantity', 'InvoiceDate', 'UnitPrice', 'CustomerID', 'Country']\n",
      "Processed data shape: (540400, 8)\n",
      "Creating categories using clustering...\n",
      "Category distribution:\n",
      "Category\n",
      "Category_14    254706\n",
      "Category_13     50216\n",
      "Category_17     25354\n",
      "Category_1      24989\n",
      "Category_11     24109\n",
      "Category_0      23910\n",
      "Category_7      20033\n",
      "Category_3      19421\n",
      "Category_9      17618\n",
      "Category_2      17545\n",
      "Category_15     15059\n",
      "Category_6       9941\n",
      "Category_16      9844\n",
      "Category_5       9676\n",
      "Category_8       5183\n",
      "Category_12      4736\n",
      "Category_10      4188\n",
      "Category_4       3872\n",
      "Name: count, dtype: int64\n",
      "Using a subset of 10000 samples for faster training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 48.0/48.0 [00:00<?, ?B/s]\n",
      "C:\\Users\\Samarth Parasnis\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Samarth Parasnis\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 5.65MB/s]\n",
      "tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 13.4MB/s]\n",
      "config.json: 100%|██████████| 483/483 [00:00<?, ?B/s] \n",
      "model.safetensors: 100%|██████████| 268M/268M [00:03<00:00, 88.0MB/s] \n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 250/250 [17:51<00:00,  4.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.6310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 32/32 [01:22<00:00,  2.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.0793, Accuracy: 0.9855\n",
      "Saving best model...\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 250/250 [19:08<00:00,  4.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.0491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 32/32 [01:37<00:00,  3.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.0413, Accuracy: 0.9925\n",
      "Saving best model...\n",
      "Training complete!\n",
      "Process completed successfully!\n",
      "Models saved in 'models/' directory\n",
      "\n",
      "Sample predictions:\n",
      "Text: peg bag apples design\n",
      "Predicted category: Category_17\n",
      "\n",
      "Text: vintage paisley stationery set\n",
      "Predicted category: Category_13\n",
      "\n",
      "Text: water damaged\n",
      "Predicted category: Category_6\n",
      "\n",
      "Text: vintage doily travel sewing kit\n",
      "Predicted category: Category_4\n",
      "\n",
      "Text: french enamel candleholder\n",
      "Predicted category: Category_14\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load the data\n",
    "def load_data(file_path):\n",
    "    \"\"\"Load and preprocess the Online_Retail.xlsx file\"\"\"\n",
    "    try:\n",
    "        # Try reading as Excel file\n",
    "        df = pd.read_excel(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading Excel file: {e}\")\n",
    "        try:\n",
    "            # If Excel reading fails, try as CSV\n",
    "            df = pd.read_excel(file_path)\n",
    "        except Exception as e2:\n",
    "            print(f\"Error reading CSV file: {e2}\")\n",
    "            raise\n",
    "\n",
    "    print(f\"Original data shape: {df.shape}\")\n",
    "\n",
    "    # Display the schema\n",
    "    print(\"Dataset columns:\")\n",
    "    print(df.columns.tolist())\n",
    "\n",
    "    # Drop rows with missing descriptions\n",
    "    if 'Description' in df.columns:\n",
    "        df = df.dropna(subset=['Description'])\n",
    "    else:\n",
    "        # Try to find a column that might contain product descriptions\n",
    "        text_columns = [col for col in df.columns if any(x in col.lower() for x in ['desc', 'prod', 'item', 'name'])]\n",
    "        if text_columns:\n",
    "            print(f\"Using '{text_columns[0]}' as the description column\")\n",
    "            df = df.rename(columns={text_columns[0]: 'Description'})\n",
    "            df = df.dropna(subset=['Description'])\n",
    "        else:\n",
    "            raise ValueError(\"Could not find a description column in the dataset\")\n",
    "\n",
    "    # Clean descriptions\n",
    "    df['Description'] = df['Description'].astype(str).apply(lambda x: clean_text(x))\n",
    "\n",
    "    # Remove empty descriptions\n",
    "    df = df[df['Description'].str.strip() != '']\n",
    "\n",
    "    print(f\"Processed data shape: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean the text data\"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters and extra spaces\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def create_categories(df, n_clusters=18):  # Reduced from 20 to 10 clusters\n",
    "    \"\"\"Create categories using clustering on TF-IDF features\"\"\"\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.cluster import MiniBatchKMeans  # Using MiniBatchKMeans instead of KMeans\n",
    "\n",
    "    print(\"Creating categories using clustering...\")\n",
    "\n",
    "    # Create TF-IDF features with fewer features\n",
    "    vectorizer = TfidfVectorizer(max_features=500, stop_words='english')  # Reduced from 1000 to 500\n",
    "    tfidf_matrix = vectorizer.fit_transform(df['Description'])\n",
    "\n",
    "    # Apply MiniBatchKMeans clustering (faster than regular KMeans)\n",
    "    kmeans = MiniBatchKMeans(n_clusters=n_clusters, random_state=42, batch_size=100)\n",
    "    df['Category'] = kmeans.fit_predict(tfidf_matrix)\n",
    "\n",
    "    # Convert numeric clusters to string categories\n",
    "    df['Category'] = 'Category_' + df['Category'].astype(str)\n",
    "\n",
    "    # Save the vectorizer and kmeans model for later use\n",
    "    import joblib\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    joblib.dump(vectorizer, 'models/tfidf_vectorizer.pkl')\n",
    "    joblib.dump(kmeans, 'models/kmeans_model.pkl')\n",
    "\n",
    "    # Print category distribution\n",
    "    print(\"Category distribution:\")\n",
    "    print(df['Category'].value_counts())\n",
    "\n",
    "    return df, vectorizer, kmeans\n",
    "\n",
    "# Create PyTorch dataset with reduced sequence length\n",
    "class ProductDataset(Dataset):\n",
    "    def __init__(self, descriptions, labels, tokenizer, max_length=64):  # Reduced from 128 to 64\n",
    "        self.descriptions = descriptions\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.descriptions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.descriptions[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def train_model(df, num_epochs=2, batch_size=32, learning_rate=5e-5):  # Increased batch size, fewer epochs\n",
    "    \"\"\"Train a DistilBERT model for category classification\"\"\"\n",
    "    # Encode categories\n",
    "    label_encoder = LabelEncoder()\n",
    "    df['label_encoded'] = label_encoder.fit_transform(df['Category'])\n",
    "\n",
    "    # Save the label encoder\n",
    "    import joblib\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    joblib.dump(label_encoder, 'models/label_encoder.pkl')\n",
    "\n",
    "    # Sample a subset of data to speed up training even more (optional)\n",
    "    if len(df) > 10000:\n",
    "        df = df.sample(10000, random_state=42)\n",
    "        print(f\"Using a subset of {len(df)} samples for faster training\")\n",
    "\n",
    "    # Split data\n",
    "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "        df['Description'].values,\n",
    "        df['label_encoded'].values,\n",
    "        test_size=0.2,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Load DistilBERT tokenizer and model (much smaller than BERT)\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(\n",
    "        'distilbert-base-uncased',\n",
    "        num_labels=len(label_encoder.classes_)\n",
    "    )\n",
    "\n",
    "    # Save the tokenizer for later use\n",
    "    tokenizer.save_pretrained('models/tokenizer')\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = ProductDataset(train_texts, train_labels, tokenizer)\n",
    "    val_dataset = ProductDataset(val_texts, val_labels, tokenizer)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size * 2  # Double batch size for validation\n",
    "    )\n",
    "\n",
    "    # Initialize optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        print(f\"Average training loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "\n",
    "                loss = outputs.loss\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs.logits, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        accuracy = correct / total\n",
    "        print(f\"Validation loss: {avg_val_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            print(\"Saving best model...\")\n",
    "            model.save_pretrained('models/distilbert_model')\n",
    "\n",
    "    print(\"Training complete!\")\n",
    "    return model, tokenizer, label_encoder\n",
    "\n",
    "def main():\n",
    "    file_path = 'Online ' \\\n",
    "    'Retail.xlsx'\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File '{file_path}' not found\")\n",
    "        return\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    df = load_data(file_path)\n",
    "    df, vectorizer, kmeans = create_categories(df)\n",
    "    model, tokenizer, label_encoder = train_model(df)\n",
    "\n",
    "    print(\"Process completed successfully!\")\n",
    "    print(f\"Models saved in 'models/' directory\")\n",
    "    sample_texts = df['Description'].sample(5).tolist()\n",
    "    print(\"\\nSample predictions:\")\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    for text in sample_texts:\n",
    "        encoding = tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=64,  # Use the same max_length as training\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        input_ids = encoding['input_ids'].to(device)\n",
    "        attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            _, predicted = torch.max(outputs.logits, 1)\n",
    "            category = label_encoder.inverse_transform([predicted.item()])[0]\n",
    "\n",
    "        print(f\"Text: {text}\")\n",
    "        print(f\"Predicted category: {category}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef70b827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded successfully. Using device: cpu\n",
      "Available categories: Category_0, Category_1, Category_10, Category_11, Category_12, Category_13, Category_14, Category_15, Category_16, Category_17, Category_2, Category_3, Category_4, Category_5, Category_6, Category_7, Category_8, Category_9\n",
      "\n",
      "Product Category Prediction Tool\n",
      "================================\n",
      "Enter product descriptions to get category predictions.\n",
      "Type 'quit', 'exit', or 'q' to exit.\n",
      "Type 'file' to load descriptions from a file.\n",
      "Type 'method' to switch between transformer and clustering methods.\n",
      "Current method: transformer\n",
      "\n",
      "Enter a product description:\n",
      "Predicted category: Category_14\n",
      "Confidence: 0.9489\n",
      "\n",
      "Enter a product description:\n",
      "Predicted category: Category_14\n",
      "Confidence: 0.9973\n",
      "\n",
      "Enter a product description:\n",
      "Predicted category: Category_14\n",
      "Confidence: 0.9489\n",
      "\n",
      "Enter a product description:\n",
      "Predicted category: Category_14\n",
      "Confidence: 0.9797\n",
      "\n",
      "Enter a product description:\n",
      "Predicted category: Category_14\n",
      "Confidence: 0.9990\n",
      "\n",
      "Enter a product description:\n",
      "Predicted category: Category_14\n",
      "Confidence: 0.9967\n",
      "\n",
      "Enter a product description:\n",
      "Predicted category: Category_14\n",
      "Confidence: 0.9983\n",
      "\n",
      "Enter a product description:\n",
      "Predicted category: Category_14\n",
      "Confidence: 0.9489\n",
      "\n",
      "Enter a product description:\n",
      "Predicted category: Category_14\n",
      "Confidence: 0.9489\n",
      "\n",
      "Enter a product description:\n",
      "Predicted category: Category_14\n",
      "Confidence: 0.9489\n",
      "\n",
      "Enter a product description:\n",
      "Predicted category: Category_14\n",
      "Confidence: 0.9489\n",
      "\n",
      "Enter a product description:\n",
      "Predicted category: Category_14\n",
      "Confidence: 0.9489\n",
      "\n",
      "Enter a product description:\n",
      "Predicted category: Category_14\n",
      "Confidence: 0.9489\n",
      "\n",
      "Enter a product description:\n",
      "Predicted category: Category_14\n",
      "Confidence: 0.9489\n",
      "\n",
      "Enter a product description:\n",
      "Predicted category: Category_14\n",
      "Confidence: 0.9489\n",
      "\n",
      "Enter a product description:\n",
      "Predicted category: Category_14\n",
      "Confidence: 0.9489\n",
      "\n",
      "Enter a product description:\n",
      "Predicted category: Category_14\n",
      "Confidence: 0.9489\n",
      "\n",
      "Enter a product description:\n",
      "Predicted category: Category_14\n",
      "Confidence: 0.9489\n",
      "\n",
      "Enter a product description:\n",
      "Predicted category: Category_14\n",
      "Confidence: 0.9489\n",
      "\n",
      "Enter a product description:\n",
      "Predicted category: Category_14\n",
      "Confidence: 0.9489\n",
      "\n",
      "Enter a product description:\n",
      "Predicted category: Category_14\n",
      "Confidence: 0.9489\n",
      "\n",
      "Enter a product description:\n",
      "Predicted category: Category_14\n",
      "Confidence: 0.9489\n",
      "\n",
      "Enter a product description:\n",
      "Predicted category: Category_14\n",
      "Confidence: 0.9489\n",
      "\n",
      "Enter a product description:\n",
      "Predicted category: Category_14\n",
      "Confidence: 0.9489\n",
      "\n",
      "Enter a product description:\n",
      "Predicted category: Category_14\n",
      "Confidence: 0.9489\n",
      "\n",
      "Enter a product description:\n",
      "Predicted category: Category_14\n",
      "Confidence: 0.9489\n",
      "\n",
      "Enter a product description:\n",
      "Predicted category: Category_14\n",
      "Confidence: 0.9489\n",
      "\n",
      "Enter a product description:\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "import re\n",
    "\n",
    "class ProductCategorizer:\n",
    "    def __init__(self, model_dir='models'):\n",
    "        \"\"\"\n",
    "        Initialize the product categorizer by loading all required models.\n",
    "\n",
    "        Args:\n",
    "            model_dir (str): Directory containing the saved models\n",
    "        \"\"\"\n",
    "        # Load DistilBERT model and tokenizer\n",
    "        self.model = DistilBertForSequenceClassification.from_pretrained(f\"{model_dir}/distilbert_model\")\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained(f\"{model_dir}/tokenizer\")\n",
    "\n",
    "        # Load label encoder\n",
    "        self.label_encoder = joblib.load(f\"{model_dir}/label_encoder.pkl\")\n",
    "\n",
    "        # Load TF-IDF vectorizer and KMeans model (for alternative clustering-based categorization)\n",
    "        self.vectorizer = joblib.load(f\"{model_dir}/tfidf_vectorizer.pkl\")\n",
    "        self.kmeans = joblib.load(f\"{model_dir}/kmeans_model.pkl\")\n",
    "\n",
    "        # Set device (GPU if available, else CPU)\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.model.eval()  # Set model to evaluation mode\n",
    "\n",
    "        print(f\"Models loaded successfully. Using device: {self.device}\")\n",
    "        print(f\"Available categories: {', '.join(self.label_encoder.classes_)}\")\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean the text data using the same preprocessing as during training\"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        # Remove special characters and extra spaces\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "\n",
    "    def predict_category(self, product_description, method='transformer'):\n",
    "        \"\"\"\n",
    "        Predict the category for a given product description.\n",
    "\n",
    "        Args:\n",
    "            product_description (str): The product description text\n",
    "            method (str): Either 'transformer' to use the DistilBERT model or 'clustering' to use TF-IDF + KMeans\n",
    "\n",
    "        Returns:\n",
    "            tuple: (predicted_category, confidence_score) for transformer method\n",
    "                   (predicted_category, None) for clustering method\n",
    "        \"\"\"\n",
    "        # Clean text\n",
    "        clean_description = self.clean_text(product_description)\n",
    "\n",
    "        if method == 'transformer':\n",
    "            # Tokenize input\n",
    "            encoding = self.tokenizer(\n",
    "                clean_description,\n",
    "                add_special_tokens=True,\n",
    "                max_length=64,\n",
    "                return_token_type_ids=False,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            # Move tensors to device\n",
    "            input_ids = encoding['input_ids'].to(self.device)\n",
    "            attention_mask = encoding['attention_mask'].to(self.device)\n",
    "\n",
    "            # Get prediction\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
    "                confidence, predicted_idx = torch.max(probabilities, 1)\n",
    "\n",
    "                # Convert prediction to category label\n",
    "                category = self.label_encoder.inverse_transform([predicted_idx.item()])[0]\n",
    "\n",
    "                return category, confidence.item()\n",
    "\n",
    "        elif method == 'clustering':\n",
    "            # Transform text using TF-IDF\n",
    "            text_vector = self.vectorizer.transform([clean_description])\n",
    "\n",
    "            # Predict cluster\n",
    "            cluster_id = self.kmeans.predict(text_vector)[0]\n",
    "            category = f\"Category_{cluster_id}\"\n",
    "\n",
    "            return category, None\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Method must be either 'transformer' or 'clustering'\")\n",
    "\n",
    "    def batch_predict(self, descriptions_list, method='transformer'):\n",
    "        \"\"\"\n",
    "        Predict categories for a list of product descriptions.\n",
    "\n",
    "        Args:\n",
    "            descriptions_list (list): List of product description strings\n",
    "            method (str): Either 'transformer' or 'clustering'\n",
    "\n",
    "        Returns:\n",
    "            pandas.DataFrame: DataFrame with descriptions and their predicted categories\n",
    "        \"\"\"\n",
    "        results = []\n",
    "\n",
    "        for desc in descriptions_list:\n",
    "            category, confidence = self.predict_category(desc, method)\n",
    "\n",
    "            result = {\n",
    "                'description': desc,\n",
    "                'category': category\n",
    "            }\n",
    "\n",
    "            if confidence is not None:\n",
    "                result['confidence'] = confidence\n",
    "\n",
    "            results.append(result)\n",
    "\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Initialize categorizer\n",
    "    categorizer = ProductCategorizer()\n",
    "\n",
    "    # Interactive mode\n",
    "    print(\"\\nProduct Category Prediction Tool\")\n",
    "    print(\"================================\")\n",
    "    print(\"Enter product descriptions to get category predictions.\")\n",
    "    print(\"Type 'quit', 'exit', or 'q' to exit.\")\n",
    "    print(\"Type 'file' to load descriptions from a file.\")\n",
    "    print(\"Type 'method' to switch between transformer and clustering methods.\")\n",
    "\n",
    "    method = 'transformer'\n",
    "    print(f\"Current method: {method}\")\n",
    "\n",
    "    while True:\n",
    "        print(\"\\nEnter a product description:\")\n",
    "        user_input = input(\"> \").strip()\n",
    "\n",
    "        if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"Exiting...\")\n",
    "            break\n",
    "\n",
    "        elif user_input.lower() == 'method':\n",
    "            if method == 'transformer':\n",
    "                method = 'clustering'\n",
    "            else:\n",
    "                method = 'transformer'\n",
    "            print(f\"Switched to {method} method\")\n",
    "            continue\n",
    "\n",
    "        elif user_input.lower() == 'file':\n",
    "            file_path = input(\"Enter the path to your CSV or Excel file: \").strip()\n",
    "\n",
    "            try:\n",
    "                if file_path.endswith('.csv'):\n",
    "                    df = pd.read_csv(file_path)\n",
    "                elif file_path.endswith(('.xlsx', '.xls')):\n",
    "                    df = pd.read_excel(file_path)\n",
    "                else:\n",
    "                    print(\"Unsupported file format. Please use CSV or Excel.\")\n",
    "                    continue\n",
    "\n",
    "                # Try to find the description column\n",
    "                desc_col = None\n",
    "                for col in df.columns:\n",
    "                    if any(x in col.lower() for x in ['desc', 'prod', 'item', 'name']):\n",
    "                        desc_col = col\n",
    "                        break\n",
    "\n",
    "                if desc_col is None and len(df.columns) > 0:\n",
    "                    desc_col = df.columns[0]  # Use first column as fallback\n",
    "\n",
    "                if desc_col:\n",
    "                    print(f\"Using column '{desc_col}' for descriptions\")\n",
    "                    descriptions = df[desc_col].astype(str).tolist()\n",
    "\n",
    "                    # Predict categories\n",
    "                    results = categorizer.batch_predict(descriptions, method)\n",
    "\n",
    "                    # Save results\n",
    "                    output_path = file_path.rsplit('.', 1)[0] + '_categorized.' + file_path.rsplit('.', 1)[1]\n",
    "\n",
    "                    # Combine with original dataframe\n",
    "                    df['predicted_category'] = results['category']\n",
    "                    if 'confidence' in results.columns:\n",
    "                        df['confidence'] = results['confidence']\n",
    "\n",
    "                    # Save to file\n",
    "                    if output_path.endswith('.csv'):\n",
    "                        df.to_csv(output_path, index=False)\n",
    "                    else:\n",
    "                        df.to_excel(output_path, index=False)\n",
    "\n",
    "                    print(f\"Results saved to {output_path}\")\n",
    "                else:\n",
    "                    print(\"Could not find a suitable column for descriptions\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file: {e}\")\n",
    "\n",
    "            continue\n",
    "\n",
    "        # Process single description\n",
    "        try:\n",
    "            category, confidence = categorizer.predict_category(user_input, method)\n",
    "\n",
    "            print(f\"Predicted category: {category}\")\n",
    "            if confidence is not None:\n",
    "                print(f\"Confidence: {confidence:.4f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
